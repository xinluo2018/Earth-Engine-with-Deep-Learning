{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainer_ai_platform_full.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM7rKy92Ge5zBbeXQdK5hNU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CdNkcVpF_MOZ","colab_type":"text"},"source":["## **Train the model on the Google AI Platform**"]},{"cell_type":"code","metadata":{"id":"Yek0InATyRNY","colab_type":"code","colab":{}},"source":["# Cloud authentication.\n","from google.colab import auth\n","auth.authenticate_user()\n","## Mount on google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# # Import and initialize the Earth Engine library.\n","import ee\n","ee.Authenticate()\n","ee.Initialize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxvaXvrZW0jP","colab_type":"text"},"source":["### <font color=red> **！！Note: the runtime-version setting (supported tensorflow version in AI platform) should be consistency with the Tensorflow version in the colab, and we use Tensorflow 2.2.0 here!**"]},{"cell_type":"code","metadata":{"id":"1x0JS0Mz6GSS","colab_type":"code","colab":{}},"source":["# !pip install tensorflow==2.2.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI2yvFUzyR-m","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/drive/My Drive/Earth-Engine-with-Deep-Learning/trainer\")\n","import tensorflow as tf\n","print(tf.__version__)\n","# Folium setup.\n","import folium\n","print(folium.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKT2EEhN_Sop","colab_type":"code","colab":{}},"source":["Package_Path = 'ai_platform_train'\n","\n","# !ls -l\n","# !mkdir {Package_Path}\n","# !touch {Package_Path}/__init__.py\n","# !ls -l {Package_Path}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4dFIn0vlBCQu","colab_type":"text"},"source":["### **Prepare the configuration file.**"]},{"cell_type":"code","metadata":{"id":"G-2y07IcA-2q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599225932348,"user_tz":-480,"elapsed":950,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"1c5832aa-4888-4787-9b05-9fff460cb2a4"},"source":["%%writefile {Package_Path}/config.py\n","\n","import tensorflow as tf\n","\n","# INSERT YOUR PROJECT HERE!\n","Project = 'my-project-20200813'\n","\n","# INSERT YOUR BUCKET HERE!\n","Bucket = 'earth-engine-bucket-1'\n","\n","# Specify names of output locations in Cloud Storage.\n","Folder = 'ai_platform_train'\n","Job_Dir = 'gs://' + Bucket + '/' + Folder\n","\n","Model_Dir = Job_Dir + '/model'\n","Logs_Dir = Job_Dir + '/logs'\n","\n","# Put the EEified model next to the trained model directory.\n","EEified_Dir = Job_Dir + '/eeified'\n","\n","# Pre-computed training data.\n","Train_Data_Folder = 'NLCD_Impervious_Data'\n","\n","# output bands\n","Bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n","Targets = ['impervious']\n","Features = Bands + Targets\n","\n","# Specify the size and shape of patches expected by the model.\n","Kernel_Size = [256, 256]\n","Columns = [\n","  tf.io.FixedLenFeature(shape=Kernel_Size, dtype=tf.float32) for k in Features\n","]\n","Features_Dict = dict(zip(Features, Columns))\n","\n","# Sizes of the training datasets.\n","Train_Size = 1000\n","\n","# Specify model training parameters.\n","Batch_Size = 16\n","Epochs = 50\n","Buffer_Size = 2000\n","Optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9)\n","Loss = 'MeanSquaredError'\n","Metrics = ['RootMeanSquaredError']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting ai_platform_train/config.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"69EHNyFsf1aT","colab_type":"text"},"source":["### Check the configuration file."]},{"cell_type":"code","metadata":{"id":"aMiKiC-kfBar","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599282806671,"user_tz":-480,"elapsed":4331,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"2da57c23-b1d2-473f-81ac-cfa754a8f300"},"source":["# !cat {PACKAGE_PATH}/config.py\n","from ai_platform_train import config\n","print('\\n', config.Job_Dir)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," gs://earth-engine-bucket-1/ai_platform_train\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oDZdsp8eh3kd","colab_type":"text"},"source":["### Data loader"]},{"cell_type":"code","metadata":{"id":"B8jGiYiKhz2C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599225942932,"user_tz":-480,"elapsed":2549,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"2a64be2e-d5f8-4892-83bb-40291d9e81f6"},"source":["## Data loader\n","%%writefile {Package_Path}/dataLoader.py\n","\n","from . import config\n","import tensorflow as tf\n","\n","# Dataset loading functions\n","def parse_tfrecord(example_proto):\n","  return tf.io.parse_single_example(example_proto, config.Features_Dict)\n","\n","def to_tuple(inputs):\n","  inputsList = [inputs.get(key) for key in config.Features]\n","  stacked = tf.stack(inputsList, axis=0)\n","  stacked = tf.transpose(stacked, [1, 2, 0])\n","  return stacked[:,:,:len(config.Bands)], stacked[:,:,len(config.Bands):]\n","\n","def get_dataset(pattern):\n","\tglob = tf.io.gfile.glob(pattern)\n","\tdataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n","\tdataset = dataset.map(parse_tfrecord)\n","\tdataset = dataset.map(to_tuple)\n","\treturn dataset\n","\n","def get_training_dataset():\n","\tglob = 'gs://' + config.Bucket + '/' + config.Train_Data_Folder + '/' + '*'\n","\tdataset = get_dataset(glob)\n","\tdataset = dataset.shuffle(config.Buffer_Size).batch(config.Batch_Size).repeat()\n","\treturn dataset\n","\n","# def get_eval_dataset():\n","# \tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.EVAL_BASE + '*'\n","# \tdataset = get_dataset(glob)\n","# \tdataset = dataset.batch(1).repeat()\n","# \treturn dataset\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting ai_platform_train/dataLoader.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cHQtXNXAmsLQ","colab_type":"text"},"source":["## Model "]},{"cell_type":"code","metadata":{"id":"wk0MPi4hmmq3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599225946967,"user_tz":-480,"elapsed":1953,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"cfc5dbfb-88ac-4505-d905-6f27270f2cf7"},"source":["## model building\n","%%writefile {Package_Path}/model.py\n","\n","import tensorflow as tf\n","\n","############## U-Net\n","###  Define the downsample function\n","##   Conv2D+BN+ReLU\n","def downsample(filters, size, apply_batchnorm=True):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    result = tf.keras.Sequential()\n","    result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","    if apply_batchnorm:\n","        result.add(tf.keras.layers.BatchNormalization())\n","#     result.add(tf.keras.layers.LeakyReLU())\n","    result.add(tf.keras.layers.ReLU())\n","    return result\n","\n","### Define the upsample function\n","##  TransposeConv2D+BN+ReLU\n","def upsample(filters, size, apply_dropout=False):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    result = tf.keras.Sequential()\n","    result.add(\n","    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                    padding='same',\n","                                    kernel_initializer=initializer,\n","                                    use_bias=False))\n","    result.add(tf.keras.layers.BatchNormalization())\n","    if apply_dropout:\n","        result.add(tf.keras.layers.Dropout(0.5))\n","    result.add(tf.keras.layers.ReLU())\n","    return result\n","\n","## Simple U-Net\n","def UNet(input_shape, nclasses=2):\n","    ## encoder of the U-Net\n","    (img_height, img_width, img_channel) = input_shape\n","    down_stack = [\n","        downsample(12, 3), # outp: (bs, img_height/2, img_width/2, 32)\n","        downsample(24, 3), # (bs, img_height/4, img_width/4, 64)\n","        downsample(48, 3), # (bs, img_height/8, img_width/8, 128)\n","        downsample(96, 3), # (bs, img_height/16, img_width/16, 256)\n","        downsample(96, 3), # (bs, img_height/32, img_width/32, 512)\n","        # downsample(96, 3), # (bs, img_height/64, img_width/64, 512)\n","        # downsample(96, 3), # (bs, img_height/128, img_width/128, 512)\n","    ]\n","\n","    ## decoder of the U-Net\n","    up_stack = [\n","        # upsample(96, 3), # outp: (bs, img_height/64, img_width/64, 1024)\n","        # upsample(96, 3), # (bs, img_height/32, img_width/32, 1024)\n","        upsample(96, 3), # (bs, img_height/16, img_width/16, 1024)\n","        upsample(48, 3), # (bs, img_height/8, img_width/8, 512)\n","        upsample(24, 3), # (bs, img_height/4, img_width/4, 256)\n","        upsample(12, 3), # (bs, img_height/2, img_width/2, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    \n","    # define the input and output tensors\n","    inputs = tf.keras.layers.Input(shape=[img_height, img_width, img_channel])\n","    if nclasses == 2:\n","        last = tf.keras.layers.Conv2DTranspose(1, 3,\n","                            strides=2,\n","                            padding='same',\n","                            kernel_initializer=initializer,\n","                            activation= 'sigmoid')  ## \n","    else:\n","        last = tf.keras.layers.Conv2DTranspose(nclasses, 3,\n","                            strides=2,\n","                            padding='same',\n","                            kernel_initializer=initializer,\n","                            activation= 'softmax')  ##\n","    concat = tf.keras.layers.Concatenate()    \n","    x = inputs\n","    # Downsampling through the model\n","    skips = []   # reserve the output of medium output of the encoder network \n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","    skips = reversed(skips[:-1])  #  \n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = concat([x, skip])\n","    x = last(x)\n","    return tf.keras.Model(inputs=inputs, outputs=x)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting ai_platform_train/model.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LOI1F5V21Tkl","colab_type":"text"},"source":["## Check the dataloader functions and the built model"]},{"cell_type":"code","metadata":{"id":"DbDT7YzorfUf","colab_type":"code","colab":{}},"source":["from ai_platform_train import model\n","from ai_platform_train import dataLoader\n","\n","trainData = dataLoader.get_training_dataset()\n","# print(iter(trainData.take(1)).next())\n","\n","model = model.UNet(input_shape=(256, 256, 6), nclasses=2)\n","# print(model.summary())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsbu7RZg1qju","colab_type":"text"},"source":["## Training task on AI Platform"]},{"cell_type":"code","metadata":{"id":"6Y-r_Xav1RpZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599225956355,"user_tz":-480,"elapsed":1973,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"bed8443e-c3bf-49c4-e079-283d52e072a1"},"source":["%%writefile {Package_Path}/trainingTask.py\n","\n","from . import config\n","from . import model\n","from . import dataLoader\n","import tensorflow as tf\n","\n","if __name__ == '__main__':\n","\n","    training = dataLoader.get_training_dataset()\n","\n","    model = model.UNet(input_shape=(256, 256, 6), nclasses=2)\n","\n","    model.compile(\n","\t\toptimizer=tf.keras.optimizers.get(config.Optimizer),\n","\t\tloss=tf.keras.losses.get(config.Loss),\n","\t\tmetrics=[tf.keras.metrics.get(metric) for metric in config.Metrics])\n","\n","    model.fit(\n","        x=training,\n","        epochs=config.Epochs, \n","        steps_per_epoch=10,\n","        callbacks=[tf.keras.callbacks.TensorBoard(config.Logs_Dir)]\n","        )\n","\n","    model.save(config.Model_Dir, save_format='tf')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting ai_platform_train/trainingTask.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"07mMoyFY4QhB","colab_type":"text"},"source":["## Submit the package to AI Platform for training"]},{"cell_type":"code","metadata":{"id":"yWXXJBirBHOm","colab_type":"code","colab":{}},"source":["import time\n","\n","Job_Name = 'impervious_unet_training_job_' + time.strftime(\"%Y%m%d%H%M\")\n","Train_Package_Path = 'ai_platform_train'\n","Main_Trainer_Module = 'ai_platform_train.trainingTask'\n","Region = 'asia-east1'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0Pl9xfz4MIi","colab_type":"code","colab":{}},"source":["!gcloud ai-platform jobs submit training {Job_Name} \\\n","    --job-dir {config.Job_Dir} \\\n","    --package-path {Train_Package_Path} \\\n","    --module-name {Main_Trainer_Module} \\\n","    --region {Region} \\\n","    --project {config.Project} \\\n","    --runtime-version 2.2 \\\n","    --python-version 3.7 \\\n","    --scale-tier basic-gpu\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmvtsGg94Mir","colab_type":"code","colab":{}},"source":["desc = !gcloud ai-platform jobs describe {Job_Name} --project {config.Project}\n","state = desc.grep('state:')[0].split(':')[1].strip()\n","print(state)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fyapfr_bEm8p","colab_type":"text"},"source":["## **Inspect the trained model and prepare the model for making predictions in *Earth* Engine**\n"]},{"cell_type":"code","metadata":{"id":"u_QBwTIYARk1","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir {config.Logs_Dir}\n"],"execution_count":null,"outputs":[]}]}