{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataLoader.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsFLsyQPe15UeSVHMzr+4e"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8uHBVmYQbH-F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1599312250328,"user_tz":-480,"elapsed":892,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"e0c4ad6d-5cc7-4039-dbfc-a8eaf3bddc96"},"source":["## Mount on google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","## to the work directory\n","import os\n","work_dir = \"/content/drive/My Drive/Earth-Engine-with-Deep-Learning/trainer/ai_platform_package\"\n","os.chdir(work_dir)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GVEW271ja2wk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599312256706,"user_tz":-480,"elapsed":1173,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"8c52d21e-ee26-4324-f07d-4fcc262441e6"},"source":["## Data loader\n","%%writefile dataLoader.py\n","\n","from . import config\n","import tensorflow as tf\n","\n","# Dataset loading functions\n","def parse_tfrecord(example_proto):\n","  return tf.io.parse_single_example(example_proto, config.Features_Dict)\n","\n","def to_tuple(inputs):\n","  inputsList = [inputs.get(key) for key in config.Features]\n","  stacked = tf.stack(inputsList, axis=0)\n","  stacked = tf.transpose(stacked, [1, 2, 0])\n","  return stacked[:,:,:len(config.Bands)], stacked[:,:,len(config.Bands):]\n","\n","def get_dataset(pattern):\n","\tglob = tf.io.gfile.glob(pattern)\n","\tdataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n","\tdataset = dataset.map(parse_tfrecord)\n","\tdataset = dataset.map(to_tuple)\n","\treturn dataset\n","\n","def get_training_dataset():\n","\tglob = 'gs://' + config.Bucket + '/' + config.Train_Data_Folder + '/' + '*'\n","\tdataset = get_dataset(glob)\n","\tdataset = dataset.shuffle(config.Buffer_Size).batch(config.Batch_Size).repeat()\n","\treturn dataset\n","\n","# def get_eval_dataset():\n","# \tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.EVAL_BASE + '*'\n","# \tdataset = get_dataset(glob)\n","# \tdataset = dataset.batch(1).repeat()\n","# \treturn dataset\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Overwriting dataLoader.py\n"],"name":"stdout"}]}]}