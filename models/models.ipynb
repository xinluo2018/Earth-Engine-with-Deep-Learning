{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPAFn4FMIz6YfbppIpDbLj2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jiPGGUDZCvhi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600510550969,"user_tz":-480,"elapsed":21647,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"e5e6739c-53f0-48dd-dee2-54e077a4d254"},"source":["# mount on google drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# go to your work patch\n","import os\n","os.chdir(\"/content/drive/My Drive/Earth-Engine-with-Deep-Learning\")\n","#!ls\n","# !nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7LSFgDdL_4NP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600510551807,"user_tz":-480,"elapsed":22468,"user":{"displayName":"Xin Luo","photoUrl":"","userId":"06301970496892076570"}},"outputId":"5f142f99-c675-41ae-af64-709611057ad7"},"source":["%%writefile models/models.py\n","\n","import tensorflow as tf\n","\n","############## U-Net\n","###  Define the downsample function\n","##   Conv2D+BN+ReLU\n","def downsample(filters, size, apply_dropout=True):\n","    result = tf.keras.Sequential()\n","    result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                        kernel_initializer='he_normal', use_bias=True))\n","    result.add(tf.keras.layers.BatchNormalization())\n","    result.add(tf.keras.layers.ReLU())\n","    result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n","                        kernel_initializer='he_normal', use_bias=True))\n","    result.add(tf.keras.layers.BatchNormalization())\n","    result.add(tf.keras.layers.ReLU())\n","    if apply_dropout:\n","        result.add(tf.keras.layers.Dropout(0.5))\n","    return result\n","\n","### Define the upsample function\n","##  TransposeConv2D+BN+ReLU\n","def upsample(filters, size, apply_dropout=True):\n","    result = tf.keras.Sequential()\n","    result.add(\n","    tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n","                        kernel_initializer='he_normal', use_bias=True))       \n","    result.add(tf.keras.layers.BatchNormalization())\n","    result.add(tf.keras.layers.ReLU())\n","    result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n","                        kernel_initializer='he_normal', use_bias=True))\n","    result.add(tf.keras.layers.BatchNormalization())\n","    result.add(tf.keras.layers.ReLU())\n","    if apply_dropout:\n","        result.add(tf.keras.layers.Dropout(0.5))\n","    return result\n","\n","## Simple U-Net\n","def UNet(input_shape, nclasses=2):\n","    ## encoder of the U-Net\n","    (img_height, img_width, img_channel) = input_shape\n","    down_stack = [\n","        downsample(32, 3), # outp: (bs, img_height/2, img_width/2, 32)\n","        downsample(64, 3), # (bs, img_height/4, img_width/4, 64)\n","        downsample(64, 3), # (bs, img_height/8, img_width/8, 128)\n","        downsample(128, 3), # (bs, img_height/16, img_width/16, 256)\n","        downsample(128, 3), # (bs, img_height/32, img_width/32, 512)\n","        downsample(256, 3), # (bs, img_height/64, img_width/64, 512)\n","        # downsample(256, 3), # (bs, img_height/128, img_width/128, 512)\n","    ]\n","\n","    ## decoder of the U-Net\n","    up_stack = [\n","        # upsample(256, 3), # output: (bs, img_height/64, img_width/64, 1024)\n","        upsample(256, 3), # (bs, img_height/32, img_width/32, 1024)\n","        upsample(128, 3), # (bs, img_height/16, img_width/16, 1024)\n","        upsample(64, 3), # (bs, img_height/8, img_width/8, 512)\n","        upsample(64, 3), # (bs, img_height/4, img_width/4, 256)\n","        upsample(32, 3), # (bs, img_height/2, img_width/2, 128)\n","    ]\n","\n","    # define the input and output tensors\n","    inputs = tf.keras.layers.Input(shape=[img_height, img_width, img_channel])\n","    \n","    if nclasses == 2:        \n","        last = tf.keras.layers.Conv2D(1, 1, strides=1, padding='same',\n","                    kernel_initializer='he_normal', activation= 'sigmoid')  ## (bs, 256, 256, 1)\n","    else:\n","        last = tf.keras.layers.Conv2D(nclasses, 1, strides=1, padding='same',\n","                    kernel_initializer='he_normal', activation= 'softmax')  ## (bs, 256, 256, 1)\n","\n","    concat = tf.keras.layers.Concatenate()    \n","    x = inputs\n","    # Downsampling through the model\n","    skips = []   # reserve the output of medium output of the encoder network \n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","    skips = reversed(skips[:-1])  #  \n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = concat([x, skip])\n","    x = upsample(32, 3)(x)\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting models/models.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b86q9BTCpP8I","colab_type":"code","colab":{}},"source":["from models.models import UNet\n","\n","model = UNet(input_shape=(256,256,6), nclasses=2)\n","model.summary()"],"execution_count":null,"outputs":[]}]}